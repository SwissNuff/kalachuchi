var D=Object.defineProperty;var L=(e,t,i)=>t in e?D(e,t,{enumerable:!0,configurable:!0,writable:!0,value:i}):e[t]=i;var m=(e,t,i)=>(L(e,typeof t!="symbol"?t+"":t,i),i);(function(){const t=document.createElement("link").relList;if(t&&t.supports&&t.supports("modulepreload"))return;for(const n of document.querySelectorAll('link[rel="modulepreload"]'))s(n);new MutationObserver(n=>{for(const o of n)if(o.type==="childList")for(const l of o.addedNodes)l.tagName==="LINK"&&l.rel==="modulepreload"&&s(l)}).observe(document,{childList:!0,subtree:!0});function i(n){const o={};return n.integrity&&(o.integrity=n.integrity),n.referrerPolicy&&(o.referrerPolicy=n.referrerPolicy),n.crossOrigin==="use-credentials"?o.credentials="include":n.crossOrigin==="anonymous"?o.credentials="omit":o.credentials="same-origin",o}function s(n){if(n.ep)return;n.ep=!0;const o=i(n);fetch(n.href,o)}})();function h(){}function E(e){return e()}function O(){return Object.create(null)}function g(e){e.forEach(E)}function W(e){return typeof e=="function"}function A(e,t){return e!=e?t==t:e!==t||e&&typeof e=="object"||typeof e=="function"}function N(e){return Object.keys(e).length===0}function C(e,t,i){e.insertBefore(t,i||null)}function S(e){e.parentNode&&e.parentNode.removeChild(e)}function F(e){return document.createElement(e)}function H(e){return Array.from(e.childNodes)}let w;function f(e){w=e}const c=[],T=[];let u=[];const P=[],j=Promise.resolve();let b=!1;function U(){b||(b=!0,j.then(z))}function v(e){u.push(e)}const y=new Set;let d=0;function z(){if(d!==0)return;const e=w;do{try{for(;d<c.length;){const t=c[d];d++,f(t),I(t.$$)}}catch(t){throw c.length=0,d=0,t}for(f(null),c.length=0,d=0;T.length;)T.pop()();for(let t=0;t<u.length;t+=1){const i=u[t];y.has(i)||(y.add(i),i())}u.length=0}while(c.length);for(;P.length;)P.pop()();b=!1,y.clear(),f(e)}function I(e){if(e.fragment!==null){e.update(),g(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(v)}}function M(e){const t=[],i=[];u.forEach(s=>e.indexOf(s)===-1?t.push(s):i.push(s)),i.forEach(s=>s()),u=t}const q=new Set;function B(e,t){e&&e.i&&(q.delete(e),e.i(t))}function K(e,t,i){const{fragment:s,after_update:n}=e.$$;s&&s.m(t,i),v(()=>{const o=e.$$.on_mount.map(E).filter(W);e.$$.on_destroy?e.$$.on_destroy.push(...o):g(o),e.$$.on_mount=[]}),n.forEach(v)}function V(e,t){const i=e.$$;i.fragment!==null&&(M(i.after_update),g(i.on_destroy),i.fragment&&i.fragment.d(t),i.on_destroy=i.fragment=null,i.ctx=[])}function Y(e,t){e.$$.dirty[0]===-1&&(c.push(e),U(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function G(e,t,i,s,n,o,l=null,R=[-1]){const p=w;f(e);const a=e.$$={fragment:null,ctx:[],props:o,update:h,not_equal:n,bound:O(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(t.context||(p?p.$$.context:[])),callbacks:O(),dirty:R,skip_bound:!1,root:t.target||p.$$.root};l&&l(a.root);let _=!1;if(a.ctx=i?i(e,t.props||{},(r,x,...$)=>{const k=$.length?$[0]:x;return a.ctx&&n(a.ctx[r],a.ctx[r]=k)&&(!a.skip_bound&&a.bound[r]&&a.bound[r](k),_&&Y(e,r)),x}):[],a.update(),_=!0,g(a.before_update),a.fragment=s?s(a.ctx):!1,t.target){if(t.hydrate){const r=H(t.target);a.fragment&&a.fragment.l(r),r.forEach(S)}else a.fragment&&a.fragment.c();t.intro&&B(e.$$.fragment),K(e,t.target,t.anchor),z()}f(p)}class Z{constructor(){m(this,"$$");m(this,"$$set")}$destroy(){V(this,1),this.$destroy=h}$on(t,i){if(!W(i))return h;const s=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return s.push(i),()=>{const n=s.indexOf(i);n!==-1&&s.splice(n,1)}}$set(t){this.$$set&&!N(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}const J="4";typeof window<"u"&&(window.__svelte||(window.__svelte={v:new Set})).v.add(J);function Q(e){let t;return{c(){t=F("main"),t.innerHTML=`<h1>SPEAK OUT AND BE HEARD</h1> <h2>Uncovering the Shadows of Gender-Based Violence Among Filipino Redditors</h2> <p>This initiative seeks to explore and expose the experiences of gender-based
    violence as shared by individuals in Filipino Reddit communities. Our
    project analyzes the nature of these experiences, investigates patterns
    based on demographics, geography, and special dates, and examines the impact
    of community engagement on these critical issues.</p> <h1>Our Research Focus üîç</h1> <h2>Overview</h2> <p>Gender-based violence remains a pervasive issue worldwide, with social media
    platforms often becoming arenas where such incidents are discussed and
    disclosed. Our project focuses on understanding the narratives around
    gender-based violence within Filipino Reddit communities, looking for
    patterns that might inform better responses and preventative measures.</p> <h2>Problem</h2> <ul><li>What is the nature of gender-based violence shared by Redditors on
      PH-based subreddits?</li> <li>Are there demographic, geographic, or date-related patterns in the
      experiences shared?</li> <li>What type of posts receive the most community engagement based on upvotes?</li></ul> <h2>Hypotheses</h2> <p>We hypothesize that posts about gender-based violence show significant
    variations in engagement and content type depending on the demographic and
    date of posting, highlighting specific times or demographics that might be
    more vulnerable.</p> <h2>Solution</h2> <p>We propose to methodically analyze Reddit posts to uncover patterns and
    insights that can help in understanding and mitigating gender-based violence
    among the Filipino Reddit community. This includes examining the frequency
    of discussions, the context in which they occur, and the impact of community
    response on these posts.</p> <h1>About Our Dataset üìñ</h1> <p>The dataset was collected by scraping posts from Philippine subreddits and
    using keywords related to various forms of gender-based violence for the
    search inputs. The scraped data had 1,900+ rows in total.</p> <h3>Description of the Data Set</h3> <p>The dataset consists of Reddit posts collected from the following
    subreddits:</p> <ul><li>alasjuicy</li> <li>MentalHealthPH</li> <li>OffmychestPH</li> <li>Philippines</li> <li>relationship_advicePH</li></ul> <p>The posts were scraped using the Python Reddit API Wrapper (praw) and
    Jupyter Notebook.</p> <h3>Data Collection Process</h3> <h4>Web Scraping and Search Inputs</h4> <p>The posts were filtered based on specific search inputs related to different
    forms of gender-based violence. The search inputs used are as follows:</p> <ol><li>abused</li> <li>sexual harassment</li> <li>Cyberbullying</li> <li>domestic violence</li> <li>emotional abuse</li> <li>physical abuse</li> <li>unsolicited pics</li> <li>stalking</li></ol> <h4>Dataset Size</h4> <p>The originally scraped data set contains:</p> <ul><li><strong>title</strong>: The title of the post.</li> <li><strong>body</strong>: The main content of the post.</li> <li><strong>link</strong>: The link URL that directs to the post.</li> <li><strong>author</strong>: Author of the post</li> <li><strong>created</strong>: Date when the post was created.</li> <li><strong>subreddit</strong>: The subreddit from which the post originated.</li> <li><strong>searchinput</strong>: The keyword used for the search input.</li></ul> <h1>Preprocessing Details üìù</h1> <h2>Objective of Preprocessing</h2> <p>Our preprocessing efforts aim to refine the raw dataset into a clean,
    analyzable format that supports reliable data analysis and insight
    generation. This process ensures data quality and facilitates effective data
    exploration.</p> <h2>Steps in Preprocessing</h2> <h3>1. Cleaning Text Data</h3> <ul><li>We strip out irrelevant characters, such as URLs, special characters, and
      formatting elements from the posts&#39; titles and bodies to focus purely on
      textual content.</li> <li>Text data is converted to a uniform case (all lower case) to ensure
      consistency across all text entries.</li></ul> <h3>2. Handling Missing Values:</h3> <ul><li>We first identify columns with missing values. In our dataset, the &#39;body&#39;
      field sometimes lacks data due to deleted or removed posts.</li> <li>For fields essential to our analysis like &#39;body&#39;, we remove entries where
      this data is missing to maintain the integrity of our analysis.</li></ul> <h3>3. Tokenization and Removal of Stop Words</h3> <ul><li>We break down the text into individual words or tokens to analyze the
      frequency and presence of certain terms related to gender-based violence.</li> <li>Common English words that do not contribute to our specific analysis (such
      as &#39;the&#39;, &#39;is&#39;, &#39;at&#39;) are removed using a predefined list from the NLTK
      library.</li></ul> <h3>4. Feature Engineering</h3> <ul><li>Where available, we extract age and gender information mentioned within
      the post to analyze demographic patterns.</li> <li>We derive several time-related features from the &#39;created&#39; timestamp,
      including year, month, day, and time of day, to explore trends over time.</li> <li>We calculate the length of each post after cleaning to quantify the amount
      of content and analyze its relation to user engagement.</li></ul> <h3>5. Categorization of Data</h3> <p>Using the initial search inputs used to scrape data, each post is tagged
    with keywords that represent the type of violence discussed, aiding in
    categorical analysis.</p> <h3>6. Engagement Metrics</h3> <ul><li>We normalize upvotes by the average upvotes per month to identify posts
      that have unusually high engagement relative to typical subreddit
      activity.</li> <li>Considering the size of each subreddit, we calculate upvotes per capita to
      determine engagement efficiency relative to the community size.</li></ul> <h3>7. Outlier Detection</h3> <p>While we recognize that outliers can skew data analysis, they often contain
    valuable insights about extreme cases or highly impactful posts. We examine
    these outliers to understand their context and decide on a case-by-case
    basis whether to include them in the final analysis.</p> <h1>Explore Our Data üí°</h1> <p><a href="https://docs.google.com/spreadsheets/d/1xt632YneUFUZrn8Lc8KlxfnWZYtL25YuKSansk992tc/edit?usp=sharing">View our preprocessed dataset here</a></p>`},m(i,s){C(i,t,s)},p:h,i:h,o:h,d(i){i&&S(t)}}}class X extends Z{constructor(t){super(),G(this,t,null,Q,A,{})}}new X({target:document.getElementById("app")});
